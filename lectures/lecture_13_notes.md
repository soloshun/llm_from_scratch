# ğŸ” **Attention Mechanism â€“ Introduction**

### ğŸ“š Resources

- [Visual Guide & Animation](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [Paper: _Attention Is All You Need_ (1706.03762)](https://arxiv.org/abs/1706.03762)

---

## ğŸ§  The 4 Types of Attention Mechanisms

- **Simplified Self-Attention**
- **Self-Attention**
- **Causal Attention**
- **Multi-Head Attention** (used in GPT)

![](../images/L13_types_att.png)

---

### ğŸ§ª Simplified Self-Attention

- A minimal, clean form of self-attention â€“ useful for intuition building.

### ğŸ§ª Self-Attention

- Adds **trainable weights**, forming the core of attention in LLMs.

### ğŸ§ª Causal Attention

- A variant of self-attention that **restricts future context**, only attending to previous and current tokens.

### ğŸ§ª Multi-Head Attention

- Allows the model to focus on **different parts of the sequence in parallel**, across various representation subspaces.

---

## â—ï¸ The Problem with Modeling Long Sequences

Before attention, sequence modeling (like translation) was handled using **RNN-based encoder-decoder** architectures:

![](../images/L13_s1.png)

> **Word-by-word translation fails.** > ![](../images/L13_s2.png) > ![](../images/L13_s3.png)

Translation requires **contextual understanding** and **grammar alignment**.

To address this:

- Use **Encoder** to process input
- Use **Decoder** to generate output
  ![](../images/L13_s4.png)
  ğŸ“¹ _[watch video](../images/L13_enc_dec1.mp4)_

Under the hood:

- Encoder processes the full input into a **context vector**
- Decoder generates output **step-by-step**, using that context

---

## ğŸ§± Encoder-Decoder with RNNs

![](../images/L13_s5.png)

### Key Process:

- Each token updates the hidden state
- Final hidden state (a summary) passed to decoder
- Decoder generates one word at a time

### ğŸ’¡ Your Insight:

> â€œThe decoder loses full contextâ€”it only sees a compressed summary from the encoder. Imagine writing an exam from just a 1-pager summary of a 6-month course... Too much gets lost!â€

---

## ğŸ˜£ Limitations of RNNs

- Decoder **only sees the last hidden state**
- No access to earlier steps of the input
- Context loss grows with sequence length

Example:

> â€œThe cat that was sitting on the mat, which was next to the dog, jumped.â€
> The word â€œjumpedâ€ depends on understanding that â€œcatâ€ is the subjectâ€”many steps back in the sentence.
> An RNN struggles with this **long-range dependency**.

---

## ğŸ¯ Solution: Attention Mechanism

- In 2014, Bahdanau et al. proposed **soft attention** in RNNs
  ([paper](https://arxiv.org/abs/1409.0473))

![](../images/L13_s6.png)

### Key Idea:

- Decoder dynamically focuses on **different parts of input** at each step
- Uses **attention weights** to decide **which input tokens matter most**

---

## âš¡ï¸ Transformer Breakthrough (2017)

- Researchers found **RNNs aren't necessary**
- Introduced the **Transformer architecture** using **self-attention**

> Reusing our earlier sentence:
> â€œThe cat... jumped.â€

At each decoding step, the model can:

- **Look back at the full input**
- **Decide what matters most** (e.g., when generating "saute", attend to "jumped")

This **dynamic focusing** allows for learning **long-range dependencies**.

![](../images/L13_s7.png)

> ğŸ’¬ â€œThe model isnâ€™t blindly aligning word 1 to word 1â€”it learns the proper alignment during training.â€

---

## ğŸ§¾ A Brief History of Attention

![](../images/L13_s7_his.png)

> ğŸ” â€œEveryone talks about transformers now, but this line of research has evolved over **43+ years**.â€

---

## ğŸ’¥ Self-Attention = Core of LLMs

![](../images/L13_s8.png)

- **Self-Attention** allows each token to **attend to all other tokens** in the sequence.
- Core to Transformer-based models like GPT.

---

## ğŸŒ€ Self-Attention: How It Works

- In **self-attention**, the model learns **relationships between words in the same input sequence**.
- Different from earlier **attention** which dealt with **two separate sequences** (encoder + decoder).

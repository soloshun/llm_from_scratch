{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1023d469",
   "metadata": {},
   "source": [
    "# **`COURAGE` IS GOING FROM `FAILURE` TO FAILURE `WITHOUT` LOSING ENTHUSIAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebdb30",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "NB: lecture note found here **[ðŸ”¹ Lecture 12 Notes ðŸ”¹](lecture_12_notes.md)**\n",
    "\n",
    "this lecture is to build the whole preprocessing pipeline from scratch\n",
    "\n",
    "basically in this lecture we're revising everything we've done so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ced9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588eb9b",
   "metadata": {},
   "source": [
    "### **Step 1: Creating tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12736ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# first load the text data\n",
    "with open(\"./data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(f\"total number of character: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f47339",
   "metadata": {},
   "source": [
    "#### **implementing word based tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e20655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed_text = re.split(r'(--|[.,\"-():;?_!\\']|\\s)', raw_text)\n",
    "preprocessed_text = [item.strip() for item in preprocessed_text if item.strip()]\n",
    "print(len(preprocessed_text))\n",
    "print(preprocessed_text[:99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022292cb",
   "metadata": {},
   "source": [
    "#### **creating vocabs**\n",
    "vocabs is the dict of all unique words with their token ids <br>\n",
    "so first get all unique words then create the vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56591d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There', 'They', 'This', 'Those']\n",
      "1132\n"
     ]
    }
   ],
   "source": [
    "# use set ds to remove all duplicate\n",
    "all_unique_words = sorted(set(preprocessed_text))\n",
    "# adding special tokens\n",
    "all_unique_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "print(all_unique_words[:99])\n",
    "print(len(all_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561f6167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# now up to creating vocabs\n",
    "vocabs = {token: token_id for token_id, token in enumerate(all_unique_words)}\n",
    "print(len(vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87459e",
   "metadata": {},
   "source": [
    "#### **using the word based tokenizer class to create vocabs and then try encoding and decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c31d9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_based_tokenizer import SimpleWordBasedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3c5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_tokenizer = SimpleWordBasedTokenizer(split_regex=r'(--|[.,\"-():;?_!\\']|\\s)')\n",
    "\n",
    "vocabs = wb_tokenizer.create_vocabs(raw_text=raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b87a400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c1cac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 643, 1051, 140, 5, 157, 250, 887, 722, 987, 899, 722, 988, 361, 524, 727, 988, 1072, 701, 549, 207, 7, 51, 1103, 1017, 663, 139, 585, 1077, 988, 602, 996, 533, 514, 360, 6, 590, 115, 712, 973, 1108, 115, 874, 521, 5, 1090, 533, 1077, 362, 568, 30, 825, 477, 115, 791, 537, 183, 7, 59, 115, 712, 0, 22, 585, 980, 549, 1098, 550, 7, 95, 169, 1123, 722, 762, 860, 766, 568, 403, 630, 7, 11, 656, 1097, 514, 969, 1108, 988, 308, 292, 707, 530, 611, 987, 672, 1052, 938, 7, 7, 7, 7, 1131]\n",
      "I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done -- just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke.... <|unk|>\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "I looked up again, and caught sight of that sketch of the donkey hanging on the wall near his bed. His wife told me afterward it was the last thing he had done--just a note taken with a shaking hand, when he was down in Devonshire recovering from a previous heart attack. Just a note! But it tells his whole history. There are years of patient scornful persistence in every line. A man who had swum with the current could never have learned that mighty up-stream stroke. . . .*\n",
    "\"\"\"\n",
    "\n",
    "enc_samp_text = wb_tokenizer.encode(sample_text)\n",
    "print(enc_samp_text)\n",
    "print(wb_tokenizer.decode(enc_samp_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0849905",
   "metadata": {},
   "source": [
    "**the class is a word based tokenizer i create to tokenize textual data... it has functions to encode and decode... let test it out by copying from `the-verdict.txt` file and test the it out and see**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641af1c1",
   "metadata": {},
   "source": [
    "#### **using Byte Pair Encoding(BPE), subwords encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9669575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenization = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "gpt2_tokenized_data = gpt2_tokenization.encode(raw_text)\n",
    "print(len(gpt2_tokenized_data))\n",
    "# print(gpt2_tokenization.decode(gpt2_tokenized_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47413ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no gr\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469]\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:101])\n",
    "print(gpt2_tokenized_data[:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7beed",
   "metadata": {},
   "source": [
    "#### **creating input-target pair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lectures.custom_dataloader import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f4dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1444,   340,    13,   314],\n",
      "        [  287,   281, 13079,   410],\n",
      "        [  262,  1109,   351,  1602],\n",
      "        [  319,  3619,   438,   505],\n",
      "        [ 2745,     6,  4686,  1359],\n",
      "        [  338, 10568,   550,   587],\n",
      "        [  262,  1396,   417,    13]])\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [  340,    13,   314,   460],\n",
      "        [  281, 13079,   410, 12523],\n",
      "        [ 1109,   351,  1602, 11227],\n",
      "        [ 3619,   438,   505,   286],\n",
      "        [    6,  4686,  1359,   319],\n",
      "        [10568,   550,   587,  2077],\n",
      "        [ 1396,   417,    13,  1675]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "context_window = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size, \n",
    "    max_length=context_window, \n",
    "    stride=90, \n",
    "    shuffle=False, \n",
    "    drop_last=True\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs,target = next(data_iter)\n",
    "print(inputs)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861f17c",
   "metadata": {},
   "source": [
    "#### **Creating Vector/token embeddings & Postional Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fb1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embeddings weights:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
      "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        ...,\n",
      "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
      "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
      "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
      "       requires_grad=True) ---> shape: torch.Size([50257, 256])\n",
      "\n",
      "positional embeddings weights:\n",
      " Parameter containing:\n",
      "tensor([[ 0.5423, -0.1224, -1.4150,  ...,  0.2515, -2.3067,  0.8155],\n",
      "        [-0.3973, -1.2575, -1.9800,  ..., -0.1207,  0.3075, -0.6422],\n",
      "        [ 0.1840,  1.1128,  1.0052,  ...,  0.2081,  0.5531, -1.1619],\n",
      "        [ 1.4155,  0.6599,  0.3760,  ...,  0.7034, -0.6108,  0.1080]],\n",
      "       requires_grad=True) ---> shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# print(token_ids)\n",
    "num_embeddings = 50257\n",
    "embeddings_dim = 256\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# token/vector embeddings\n",
    "token_embeddings_layer = torch.nn.Embedding(\n",
    "    num_embeddings=num_embeddings, embedding_dim=embeddings_dim\n",
    ")\n",
    "# positional embeddings\n",
    "positional_embeddings_layer = torch.nn.Embedding(\n",
    "    num_embeddings=context_window, embedding_dim=embeddings_dim\n",
    ")\n",
    "print(f\"token embeddings weights:\\n {token_embeddings_layer.weight} ---> shape: {token_embeddings_layer.weight.shape}\")\n",
    "print(f\"\\npositional embeddings weights:\\n {positional_embeddings_layer.weight} ---> shape: {positional_embeddings_layer.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74feb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetching vector embedding for a particular token id\n",
    "inputs.shape\n",
    "# inputs\n",
    "token_embeddings = token_embeddings_layer(inputs)\n",
    "pos_embeddings = positional_embeddings_layer(torch.arange(context_window))\n",
    "token_embeddings.shape, pos_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bad683",
   "metadata": {},
   "source": [
    "#### **Creating input embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665cfdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4784,  0.2094, -1.3080,  ...,  0.7864, -3.1091, -1.5083],\n",
       "          [-0.7497, -0.9066, -0.9927,  ..., -1.9672, -1.3960, -0.3200],\n",
       "          [ 1.1857,  2.0427, -0.2581,  ..., -1.0175,  1.6710, -1.0276],\n",
       "          [ 2.2151,  2.9436, -0.2765,  ..., -0.4182, -0.1402,  0.2612]],\n",
       " \n",
       "         [[ 0.2149, -0.5304, -0.2393,  ...,  0.1862, -1.5616,  0.8510],\n",
       "          [ 0.7812, -1.5820, -2.6241,  ..., -0.5137, -0.7331, -2.0332],\n",
       "          [-0.2703,  1.6192,  2.1158,  ...,  1.5835,  1.3515,  0.7526],\n",
       "          [ 1.7331,  1.5139,  2.5789,  ...,  2.3633, -0.8443,  0.7442]],\n",
       " \n",
       "         [[ 0.5271,  1.2655, -1.1813,  ..., -1.2999,  1.2789,  2.3797],\n",
       "          [-0.9960, -2.1080, -2.6604,  ..., -0.1250, -0.8186, -1.0254],\n",
       "          [ 0.2334,  0.8604, -0.2017,  ...,  0.2957,  1.7280, -0.9220],\n",
       "          [ 0.9077, -0.1305,  1.1129,  ...,  0.3097,  0.4268,  1.2195]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1220, -1.1855, -1.1986,  ..., -0.2260, -3.1977,  0.7988],\n",
       "          [ 0.3985, -0.7225, -1.0372,  ...,  1.2060, -0.9040,  0.2008],\n",
       "          [-0.0071,  1.6889,  0.7931,  ..., -0.0855, -0.1931,  0.8139],\n",
       "          [ 2.2143,  0.1709, -0.0259,  ...,  2.0344, -2.3029,  0.9420]],\n",
       " \n",
       "         [[-1.8668, -0.6578, -2.2008,  ..., -0.0162, -2.5080,  2.0778],\n",
       "          [-0.7459, -1.7220, -2.3580,  ...,  0.4524,  1.6597, -0.2947],\n",
       "          [ 0.4109,  3.0507,  2.5790,  ...,  1.1199,  0.4274, -0.6700],\n",
       "          [ 2.6399,  1.0694, -0.0856,  ...,  2.8044, -0.7448, -0.3827]],\n",
       " \n",
       "         [[ 0.5702, -1.5815, -1.2367,  ..., -1.4820, -1.3721,  0.5733],\n",
       "          [-2.7737, -0.6650, -2.8561,  ...,  2.1769, -0.7327,  0.8533],\n",
       "          [ 0.2191,  2.2813,  1.3284,  ...,  1.5814,  1.7174, -1.3782],\n",
       "          [ 0.9612,  1.1662,  1.4866,  ...,  2.0788,  0.1877,  2.0224]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([8, 4, 256]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_embeddings = token embeddings + positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings, input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c0f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sophia-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
